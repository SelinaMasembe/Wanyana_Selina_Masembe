{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177aef27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ARTIFICIAL INTELLIGENCE\n",
    "\"\"\"\n",
    "Machine learning->focus is put on building models and algorithms\n",
    "For all this to happen,one needs to have data (Secondary and Primary)\n",
    "\n",
    "BASICS OF MACHINE LEARNING:\n",
    "1.Data Processing:\n",
    "2.Feature Selection: selecting the most relevant features from the dataset\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "ALGORITHMS THAT WE USE IN FEATURE SELECTION:\n",
    "They are categorized into 3:\n",
    "1.Filter Methods:-independent evaluation of each feature with target variable.\n",
    "   - These methods use statistical techniques to evaluate the relevance of features.\n",
    "   - Examples include correlation coefficients, chi-square tests, and mutual information.\n",
    "   \n",
    "   Techniques under filter methods:\n",
    "    a.-Information Gain: Measures the reduction in entropy(disoreder or disorganization) or uncertainty about the target variable when a feature is known.\n",
    "    {if your model is disorganized,then you need to achieve a higher accuracy by selecting the right features}\n",
    "    Entropy: Measures the impurity or disorder in a dataset.(used in decision trees)\n",
    "\n",
    "    b.-Chi-Square Test: Assesses the independence(or relationship btn variables) of categorical features from the target variable.\n",
    "    \n",
    "    c.-Fisher's Score: Evaluates the relationship between categorical features and the target variable using statistical tests.(to aid feature selection)\n",
    "    \n",
    "    ADVANTAGES:\n",
    "    -They are easy to implement and computationally efficient. \n",
    "    \n",
    "2.Wrapper Methods/ Greedy algorithms: NOT ONE FEATURE AT A TIME.\n",
    "   - These methods [evaluate subsets of features] by training a model and assessing its performance.\n",
    "    - Examples include recursive feature elimination (RFE) and forward/backward selection.\n",
    "    Recursive feature elimination (RFE):you eliminate the least important features recursively until you reach a desired number of features.\n",
    "    Backward Selection: Starts with all features and removes the least significant ones iteratively.\n",
    "    Forward Selection: Starts with no features and adds the most significant ones iteratively.\n",
    "    \n",
    "    ADVANTAGES:\n",
    "    - They consider feature interactions and can lead to better model performance.\n",
    "    - They can be more accurate than filter methods.\n",
    "    -Can capture feature interactions and dependencies.\n",
    "    DISADVANTAGES:\n",
    "    - They can be computationally expensive, especially with large datasets.\n",
    "\n",
    "3.Embedded Methods: combines both filter and wrapper methods.\n",
    "   -Also referred to as hybrid methods.\n",
    "   - These methods perform feature selection as part of the model training process.\n",
    "    -integrate feature selection into the model training process.\n",
    "    \n",
    "    ADVANTAGES:\n",
    "    - They are computationally efficient and can handle large datasets.\n",
    "    - More efficient than wrapper methods.\n",
    "    \n",
    "    LIMITATIONS:\n",
    "    -Work best with specific algorithms and may not generalize well across different models.\n",
    "    \n",
    "    Techniques under embedded methods:\n",
    "    a.-Lasso Regression: Uses L1 regularization to shrink some feature coefficients to zero, effectively selecting a subset of features.[use decision trees]\n",
    "      -allows for sparse solutions [SPACITY] by penalizing the absolute size of coefficients.\n",
    "      -Spacity: refers to the property of a model where many coefficients are exactly zero, indicating that those features are not used in the model.\n",
    "      -Regression: a statistical method used to model the relationship between a [dependent variable] and one or more independent variables.\n",
    "      -Regression is very important in Supervised Learning, where the goal is to predict a continuous target variable based on input features.\n",
    "      \n",
    "      random forest: an ensemble learning method that combines multiple decision trees to improve predictive accuracy and control overfitting.\n",
    "      \n",
    "    b.Gradient boosting: an ensemble technique that builds models sequentially, where each new model corrects the errors of the previous ones.\n",
    "    c.-Decision Trees: Use feature importance scores to select relevant features during the tree-building process.\n",
    "\n",
    "\"\"\" \n",
    "\n",
    "# CHOOSING THE RIGHT METHOD:\n",
    "\"\"\"\n",
    "Depends on:\n",
    "    the dataset size, feature characteristics, and model requirements(especially for embedded methods).\n",
    "    -Filter methods are often a good starting point for quick feature selection, while wrapper and embedded methods can provide more accurate results but may require more computational resources.\n",
    "    -Filter methods are often faster and more scalable, while wrapper methods can be more accurate but\n",
    "    computationally expensive.\n",
    "    -Embedded methods are a good compromise, integrating feature selection into the model training process.\n",
    "    -The choice of method depends on the specific problem, dataset size, and computational resources available\n",
    "\"\"\"\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
